- Introduction missing

- In 3.2, assuming the formula h_t+1 = sigma (W h_t + V x_t+1) is already mentioned in the intro (with equation name).

- References all over the place.

- Most RRNs? section 3.2 

- On sec 3.2, assuming already talked about why relus are nice on RNNs with unitary matrices, but why haven't been used in many RNNs.

- On the intro, should talk about additive growth of hidden activations via the bias and inputs

- Sec 3.3 maybe speak more about init of thetas.

- Abstract vanishing / exp gradients and long term memory sentence too long.

- Lemma maybe not in the right place
