\documentclass{article} % For LaTeX2e
\usepackage{iclr2016_conference,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\title{\scalebox{0.95}{Unitary Evolution Recurrent Neural Networks}}


\author{Martin Arjovsky \thanks{Indicates first authors. Ordering determined by coin flip.} \\
Universidad de Buenos Aires\\
\texttt{\{marjovsky\}@dc.uba.ar} \\
\And
Amar Shah$^*$ \\
Cambridge University \\
\texttt{\{as793\}@cam.ac.uk} \\
\AND
Yoshua Bengio \\
Universite de Montr\'eal, CIFAR Senior Fellow\\
\texttt{\{yoshua.bengio\}@gmail.com} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\matr}[1]{\mathbf{#1}}
\newcommand\RR{\mathbb{R}}
\newtheorem{lemma}{Lemma}

%\iclrfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
Recurrent neural networks (RNNs) are notoriously difficult to train. When the eigenvalues of the hidden to hidden weight matrix
deviate from absolute value 1, optimization becomes difficult due to the well studied issue of vanishing and exploding gradients, especially when trying to learn long-term dependencies.
To circumvent this problem, we propose a new architecture that learns a unitary weight matrix, with eigenvalues
of absolute value exactly 1. We construct an expressive unitary weight matrix by composing several structured matrices that act
as building blocks with parameters to be learned. Optimization of this parameterization becomes feasible only when considering hidden
states in the complex domain. We demonstrate the potential of this architecture by achieving state of the art in several hard tasks
involving very long-term dependencies.

\end{abstract}
\bibliography{iclr2016_conference}
\bibliographystyle{iclr2016_conference}

\section{Unitary Evolution RNNs}

The most important feature of unitary and orthogonal matrices for our purpose is the fact that they have eigenvalues $\lambda_j$ with absolute value 1. This means that we can rewrite them as $\lambda_j = e^{i w_j}$ with $w_j \in \RR $ . A naive implementation to learn a matrix of this kind would be to fix a basis of eigenvectors $\matr{V} \in \mathbb{C}^{n \times n}$ and have

$$ \matr{W} = \matr{V} \matr{D} \matr{V}^{*} $$

where $\matr{D}$ is a diagonal such that $\matr{D}_{j,j} = e^{i w_j}$. By restricting the choice of $\matr{V}$ and tying the $w_j$s to create conjugates, $\matr{W}$ can be restricted to be a real matrix. This is evidenced by the following lemma:

\begin{lemma}
  A complex square matrix $\matr{W}$ is unitary if and only if it has an eigendecomposition of the form $\matr{W} = \matr{V} \matr{D} \matr{V}^*$. Here, $\matr{V}, \matr{D} \in \mathbb{C}^{n \times n}$ are complex matrices, where $\matr{V}$ is unitary, and $\matr{D}$ is a diagonal such that $|\matr{D}_{j,j}|=1$. Furthermore, $\matr{W}$ is a real orthogonal matrix if and only if for every eigenvalue $\matr{D}_{j,j} = \lambda_j$ with eigenvector $v_j$, there is also am eigenvalue $\lambda_k = \overline{\lambda_j}$ with corresponding eigenvector $v_k = \overline{v_j}$
\end{lemma}
\begin{proof}
  See \ref{linalgbook}
\end{proof}

To achieve a real orthogonal matrix $\matr{W}$ we would therefore have $\matr{V}$ as a fixed unitary matrix, and for every $v_j$ a column of $\matr{V}$ have also it's conjugate $v_k = \overline{v_j}$. Furthermore, for every weight $w_j$ we should also tie $w_k=-w_j$. This way $e^{i w_j} = \overline{e^{i w_k}}$ and the real orthogonality of $\matr{W}$ follows from the lemma. Since $w_j$ are real numbers and the cost is differentiable with respect to them, we can learn these weights by gradient descent. 

However, this approach has one major problem. Mainly, the memory needed to store $\matr{V}$ is $\mathcal{O}\left(n^2\right)$. Also, if $u$ is a vector, calculating $\matr{V}u$ has cost $\mathcal{O}\left( n^2 \right)$. This would lead to a cost of $\mathcal{O} \left( n^2 \right)$ for forward and backpropagation. However, we are only learning $\mathcal{O}(n)$ parameters, which is unacceptable.

If we didn't have $\matr{V}$ then we would just be learning a diagonal matrix, which likely isn't sufficient. However, since the product of unitary matrices is itself unitary, we can construct $\matr{W}$ by combining several simple matrices that are efficient to store and to operate. In \ref{dfc} they do a similar construction to reduce the amount of parameters by more than an order of magnitude in an industrial sized network, while maintaining performance. This, combined with earlyer work by \ref{structured_mat} suggests that it's possible to create very expressive matrices with relatively little cost. The building blocks we use are as follows:

\begin{itemize}
  \item $\matr{D}$ is a diagonal matrix with entries $\matr{D}_{j,j} = e^{i w_j}$ Since $\matr{D}$ is a diagonal with that have absolute value 1, it's clearly unitary. The weights $w_j \in \RR$ are going to be learned by gradient descent.
  \item $\matr{R} = \matr{I} - 2 \frac{v v^*}{\|v\|^2}$ is a reflection matrix by the complex vector $v \in \mathbb{C}^n$. This matrix is also learned, by doing gradient descent on the real and imaginary parts of $v$. The fact that this matrix is unitary is left as an exercise to the reader.
  \item $\matr{\Pi}$ a fixed random permutation. Since the transpose is the inverse permutation, it follows that it's unitary.
  \item $\mathcal{F}$ and $\mathcal{F}^{-1}$ the Fourier and inverse Fourier transforms, which have long been showed to be unitary.
\end{itemize}

For the case of $\matr{D}, \matr{R}, \matr{\Pi}$, storing each of this matrices has cost $\mathcal{O}(n)$. For the diagonal, we only store the weights $w_j$, for the reflection only the vector $v$ and for the permutation we save it as an array. Multiplying any of this matrices with a vector is also $\mathcal{O}(n)$, and is easily checked. The Fourier transforms don't need to be stored, and multiplying a vector with them can be done in $\mathcal{O}(n \log n)$ via the fast Fourier transform.

We call any architecture that uses a unitary hidden to hidden matrix a unitary evolution RNN (uRNN). After trying different architectures, the uRNN we settled on has the form

$$ \matr{W} = \matr{D_3} \matr{R_2} \mathcal{F}^{-1} \matr{D}_2 \matr{\Pi} \matr{R_1} \mathcal{F} \matr{D}_1 $$

The main issue of most of this matrices (except the permutation) is that they are complex. However, they are parameterized with real numbers, so if the final cost is real and is differentiable with respect to them, we can do gradient descent nonetheless.
\end{document}
