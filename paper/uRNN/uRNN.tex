\documentclass{article} % For LaTeX2e
\usepackage{iclr2016_conference,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{color}

\title{\scalebox{0.95}{Unitary Evolution Recurrent Neural Networks}}


\author{Martin Arjovsky \thanks{Indicates first authors. Ordering determined by coin flip.} \\
Universidad de Buenos Aires\\
\texttt{\{marjovsky\}@dc.uba.ar} \\
\And
Amar Shah$^*$ \\
Cambridge University \\
\texttt{\{as793\}@cam.ac.uk} \\
\AND
Yoshua Bengio \\
Universite de Montr\'eal, CIFAR Senior Fellow\\
\texttt{\{yoshua.bengio\}@gmail.com} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\matr}[1]{\mathbf{#1}}
\newcommand\RR{\mathbb{R}}
\newcommand\CC{\mathbb{C}}
\newtheorem{lemma}{Lemma}

%\iclrfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
Recurrent neural networks (RNNs) are notoriously difficult to train. When the eigenvalues of the hidden to hidden weight matrix
deviate from absolute value 1, optimization becomes difficult due to the well studied issue of vanishing and exploding gradients, especially when trying to learn long-term dependencies.
To circumvent this problem, we propose a new architecture that learns a unitary weight matrix, with eigenvalues
of absolute value exactly 1. We construct an expressive unitary weight matrix by composing several structured matrices that act
as building blocks with parameters to be learned. Optimization of this parameterization becomes feasible only when considering hidden
states in the complex domain. We demonstrate the potential of this architecture by achieving state of the art in several hard tasks
involving very long-term dependencies.

\end{abstract}
\bibliography{iclr2016_conference}
\bibliographystyle{iclr2016_conference}

\section{Introduction}

Some stuff

\section{Unitary Evolution RNNs}

The most important feature of unitary and orthogonal matrices for our purpose is that they have eigenvalues $\lambda_j$ with absolute value 1 (we therefore write $\lambda_j = e^{i w_j}$ with $w_j \in \RR$). A naive method to learn a unitary matrix would be to fix a basis of eigenvectors $\matr{V} \in \mathbb{C}^{n \times n}$ and set

$$ \matr{W} = \matr{V} \matr{D} \matr{V}^{*} $$

where $\matr{D}$ is a diagonal such that $\matr{D}_{j,j} = e^{i w_j}$. By restricting the choice of $\matr{V}$ and tying the learned $w_j$s to create conjugates, $\matr{W}$ can be restricted to be a real matrix. This is evident from the following lemma:

\begin{lemma}
  A complex square matrix $\matr{W}$ is unitary if and only if it has an eigendecomposition of the form $\matr{W} = \matr{V} \matr{D} \matr{V}^*$. Here, $\matr{V}, \matr{D} \in \mathbb{C}^{n \times n}$ are complex matrices, where $\matr{V}$ is unitary, and $\matr{D}$ is a diagonal such that $|\matr{D}_{j,j}|=1$. Furthermore, $\matr{W}$ is a real orthogonal matrix if and only if for every eigenvalue $\matr{D}_{j,j} = \lambda_j$ with eigenvector $v_j$, there is also an eigenvalue $\lambda_k = \overline{\lambda_j}$ with corresponding eigenvector $v_k = \overline{v_j}$
\end{lemma}
\begin{proof}
  See \ref{linalgbook}
\end{proof}

To achieve a real orthogonal matrix $\matr{W}$ we require a fixed unitary matrix $\matr{V}$, whose columns come in complex conjugate pairs $v_k = \overline{v_j}$. Similarly, we should also tie $w_k=-w_j$ in order to achieve $e^{i w_j} = \overline{e^{i w_k}}$. $\matr{W}$ is subsequently real and orthogonal by the lemma. Since $w_j$ are real numbers and if the cost is differentiable with respect to them, we can learn these weights by gradient descent. 

However, this approach has one major problem. Mainly, the memory needed to store $\matr{V}$ is $\mathcal{O}\left(n^2\right)$. Also, if $u$ is a vector, calculating $\matr{V}u$ has cost $\mathcal{O}\left( n^2 \right)$. This would lead to a cost of $\mathcal{O} \left( n^2 \right)$ for forward and backpropagation. However, we are only learning $\mathcal{O}(n)$ parameters, which is very inefficient.

If we didn't have $\matr{V}$ then we would just be learning a diagonal matrix, which likely isn't sufficient. However, since the product of unitary matrices is itself unitary, we can construct $\matr{W}$ by combining several simple structured unitary matrices that are efficient to store and to operate with. In \ref{dfc} they do a similar construction to reduce the amount of parameters by more than an order of magnitude in an industrial sized network, while maintaining performance. This, combined with earlier work by \ref{structured_mat} suggests that it is possible to create very expressive matrices with relatively little cost. The building blocks we use are as follows:

\begin{itemize}
  \item $\matr{D}$ is a diagonal matrix with entries $\matr{D}_{j,j} = e^{i w_j}$ Since $\matr{D}$ is a diagonal with that have absolute value 1, it is clearly unitary. The weights $w_j \in \RR$ are going to be learned by gradient descent.
  \item $\matr{R} = \matr{I} - 2 \frac{v v^*}{\|v\|^2}$ is a reflection matrix by the complex vector $v \in \mathbb{C}^n$. This matrix is also learned, by doing gradient descent on the real and imaginary parts of $v$. The fact that this matrix is unitary is left as an exercise to the reader.
  \item $\matr{\Pi}$ a fixed random permutation. Since the transpose is the inverse permutation, the unitary condition follows.
  \item $\mathcal{F}$ and $\mathcal{F}^{-1}$ the Fourier and inverse Fourier transforms, which have long been showed to be unitary.
\end{itemize}

For the case of $\matr{D}, \matr{R}, \matr{\Pi}$, storing each of this matrices has cost $\mathcal{O}(n)$. For the diagonal, we only store the weights $w_j$, for the reflection only the vector $v$ and for the permutation we save it as an array. Multiplying any of this matrices with a vector is also $\mathcal{O}(n)$. The Fourier transforms don't need to be stored, and multiplying a vector with them can be done in $\mathcal{O}(n \log n)$ via the Fast Fourier Transform. This means that the final cost of doing forward and backpropagation through the weight matrix is $\mathcal{O} (n \log n)$, with memory cost $\mathcal{O} (n)$ with $\mathcal{O}(n)$ parameters and hidden units.

One major advantage of this parameterization, is that the number of parameters, memory and computational cost increase linearly as a function of the hidden layer size. Therefore, we can potentially have inmense hidden layers, while this would be impossible in traditional RNNs. Especially for modelling long term dependencies, this is not a minor feature, because it increases the amount of information that can be carried from one time to another. Early work by \ref{bengio 1994} shows that having a big memory may be a crucial aspect of solving the difficulties in modeling long-term dependencies.

We call any architecture that uses a unitary hidden to hidden matrix a unitary evolution RNN (uRNN). After trying different combinations, the uRNN we settled on has the weight matrix

$$ \matr{W} = \matr{D_3} \matr{R_2} \mathcal{F}^{-1} \matr{D}_2 \matr{\Pi} \matr{R_1} \mathcal{F} \matr{D}_1 $$

The main issue of most of this matrices (except the permutation) is that they are complex. However, they are parameterized with real weights, so if the final cost is real and is differentiable with respect to them, we can do gradient descent nonetheless. In the following section we specify the complete architecture of the network, and explain how the potential difficulties of using complex hidden units can be easilly bypassed.

\section{Architecture details}

It is not enough to say how to parameterize a unitary weight matrix and get done with it. In this section, we describe the rest of the architecture, including the nonlinearity we used, how we go from real input to complex hidden units and from these units to the output. Finally, we display some implementation considerations, including memory optimization on GPUs.

\subsection{Complex hidden units}

All the hidden units are represented internally with real numbers, as imaginary and complex parts. This way, it is far more easy to code, and we completely avoid the lack of support for complex numbers by most deep learning frameworks. One of the most important examples is when multiplying the weight matrix $\matr{W} = \matr{A} + i \matr{B}$ by the complex hidden vector $h = x + i y$ In fact, it is easy to show that $ \matr{W}h = (\matr{A}x - \matr{B}y) + i (\matr{A}y + \matr{B}x) $, which leads to


$$ \begin{pmatrix} Re(\matr{W}h) \\ Im(\matr{W}h) \end{pmatrix}  = \begin{pmatrix} \matr{A} & -\matr{B} \\ \matr{B} & \ \ \ \matr{A} \end{pmatrix} \begin{pmatrix} Re(h) \\ Im(h) \end{pmatrix}$$

More generally, let $f: \CC^n \rightarrow \CC^n$ be any complex function and $z = x + i y$ any complex vector, it can be decompposed as $ f(z) = u(x, y) + i v(x, y) $ where $u,v$ are the real and imaginary parts of $f$. Therefore, we can consider the mapping as $f: \RR^{2n}  \rightarrow \RR^{2n}$, and implement everything with real numbers. Doing this, any Deep Learning framework with automatic differentiation like Theano \ref{Theano} is able to take care of computing the derivatives efficiently.

\subsection{Input to hidden and the nonlinearity}

As is the case with most recurrent networks, our uRNN follows the same hidden to hidden mapping as (1). For the input matrix we have $\matr{V} \in \CC^{n \times n}$, and it's real and complex parts are learned as parameters. For completeness, the initial hidden state $h_0 \in \CC^n$ is also learned 

Sadly, it is not as easy to choose a good nonlinearity. As discussed in section 1, using a ReLU is the natural choice when having an orthogonal or unitary RNN. Previous work on complex RNNs usually have applied a nonlinearity on both the real and imaginary parts \ref{Old CRNN papers}. However, as we will show later, this usually results on poor performance. The interpetation of why this happens is that applying a nonlinearity on the real and imaginary parts brutally changes the phase of a complex number. Since unitary matrices can be interpreted as multiplying by a phase in each eigenvector direction, this phase might be important when recovering information about past memories. Therefore, we employ a variation of the ReLU that we deem modReLU, that is applied only on the absolute value, and is defined as follows.

$$ \sigma: \CC \rightarrow \CC $$

$$ \sigma (z) = 
\left\{
  \begin{array}{ll}
    (|z|+b|) \frac{z}{|z|}  & \mbox{if } |z| + b \geq 0 \\
    0 & \mbox{if } |z| + b < 0
  \end{array}
\right.
$$


Where $b \in \RR$ is the bias of the nonlinearity, which is learned for each complex unit. It is important to note that $\sigma(z) = \text{ReLU}(|z| + b) \frac{z}{|z|}$

\subsection{Hidden to output}

For the output we have a matrix $\matr{U} \in \RR^{2n_h \times n_o}$ where $n_h$ is the hidden layer size and $n_o$ is the output size. The output is calculated as

$$ o_t = \matr{U} \begin{pmatrix} Re(h_t) \\ Im(h_t) \end{pmatrix} + b_o $$

with $b_o \in \RR^{n_o}$ the output bias. Since this output is real, we can now use it to compare the cost in the classical way with any loss function (e.g. plug it into a softmax for classification).
\subsection{Initialization}

We experimented with several initialization strategies. However, because of the stability of the network, initialization doesn't seem to be extremely important. However, the following ones did seem to make optimization marginally easier, and give a few insights on the way the netwrok works.

\begin{itemize}
  \item We initialize $\matr{V}$ and $\matr{U}$ (the input and output matrices) as in \ref{initpaper}.
  \item The biases are initialized to 0. This way at the beginning, the nonlinearity is silent. This is because the input of the ReLU is always positive (it is an absolute value). Therefore, we start with a linear unitary mapping with additive contributions from the input, which is very controled, and seems to help early optimization \ref{IRRN?}.
  \item The reflection vectors for $\matr{R}_1$ and $\matr{R}_2$ are initialized coordinate-wise from a uniform $\mathcal{U}[-1, 1]$. It's important to remark the fact that since the reflection matrix is independent from the scale of the vector, so any centered uniform will result in the same network.
  \item The diagonal weights $w_j$ are sampled from a uniform $\mathcal{U}[-\pi, \pi]$. This way $\matr{D}_{j,j} = e^{i w_j}$ is sampled uniformly from the unit circle.
  \item We initialize $h_0$ with a uniform $\mathcal{U}[-\sqrt{\frac{3}{2n_h}}, \sqrt{\frac{3}{2n_h}}]$. This way $\mathbb{E}\left[\|h_0\|^2\right] = 1$. Since the norm of the hidden units is roughly preserved because of the unitary weight matrix and inputs are usually whitened to have norm 1, we have hidden states and inputs on the same order of magnitude, which seems to help optimization \ref{batchnorm?}.
\end{itemize}

\
\end{document}
